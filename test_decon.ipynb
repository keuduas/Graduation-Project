{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36a34d27-91ce-4db7-b578-de015307390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "class Vgg16Conv(nn.Module):\n",
    "    \"\"\"\n",
    "    vgg16 convolution network architecture\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_cls=1000):\n",
    "        \"\"\"\n",
    "        Input\n",
    "            number of class, default is 1k.\n",
    "        \"\"\"\n",
    "        super(Vgg16Conv, self).__init__()\n",
    "    \n",
    "        self.features = nn.Sequential(\n",
    "            # conv1\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2, return_indices=True),\n",
    "            \n",
    "            # conv2\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2, return_indices=True),\n",
    "\n",
    "            # conv3\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2, return_indices=True),\n",
    "\n",
    "            # conv4\n",
    "            nn.Conv2d(256, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2, return_indices=True),\n",
    "\n",
    "            # conv5\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_cls),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        # index of conv\n",
    "        self.conv_layer_indices = [0, 2, 5, 7, 10, 12, 14, 17, 19, 21, 24, 26, 28]\n",
    "        # feature maps\n",
    "        self.feature_maps = OrderedDict()\n",
    "        # switch\n",
    "        self.pool_locs = OrderedDict()\n",
    "        # initial weight\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        initial weights from preptrained model by vgg16\n",
    "        \"\"\"\n",
    "        vgg16_pretrained = models.vgg16(pretrained=True)\n",
    "        # fine-tune Conv2d\n",
    "        for idx, layer in enumerate(vgg16_pretrained.features):\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                self.features[idx].weight.data = layer.weight.data\n",
    "                self.features[idx].bias.data = layer.bias.data\n",
    "        # fine-tune Linear\n",
    "        for idx, layer in enumerate(vgg16_pretrained.classifier):\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                self.classifier[idx].weight.data = layer.weight.data\n",
    "                self.classifier[idx].bias.data = layer.bias.data\n",
    "    \n",
    "    def check(self):\n",
    "        model = models.vgg16(pretrained=True)\n",
    "        return model\n",
    "\n",
    "    def forward(self, x):\n",
    "        for idx, layer in enumerate(self.features):\n",
    "            if isinstance(layer, nn.MaxPool2d):\n",
    "                x, location = layer(x)\n",
    "                # self.pool_locs[idx] = location\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        \n",
    "        # reshape to (1, 512 * 7 * 7)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        output = self.classifier(x)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47b3fd6b-0c88-4fae-8bd8-4d7f181294e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/environment/miniconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/environment/miniconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = models.vgg16(pretrained=True)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24a36773-ff56-4d98-9c29-1a47d2249db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "class Vgg16Deconv(nn.Module):\n",
    "    \"\"\"\n",
    "    vgg16 transpose convolution network architecture\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Vgg16Deconv, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            # deconv1\n",
    "            nn.MaxUnpool2d(2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 512, 3, padding=1),\n",
    "\n",
    "            # deconv2\n",
    "            nn.MaxUnpool2d(2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 256, 3, padding=1),\n",
    "            \n",
    "            # deconv3\n",
    "            nn.MaxUnpool2d(2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, 3, padding=1),\n",
    "            \n",
    "            # deconv4\n",
    "            nn.MaxUnpool2d(2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 3, padding=1),\n",
    "            \n",
    "            # deconv5\n",
    "            nn.MaxUnpool2d(2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 3, 3, padding=1)    \n",
    "        )\n",
    "\n",
    "        self.conv2deconv_indices = {\n",
    "                0:30, 2:28, 5:25, 7:23,\n",
    "                10:20, 12:18, 14:16, 17:13,\n",
    "                19:11, 21:9, 24:6, 26:4, 28:2\n",
    "                }\n",
    "\n",
    "        self.unpool2pool_indices = {\n",
    "                26:4, 21:9, 14:16, 7:23, 0:30\n",
    "                }\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        vgg16_pretrained = models.vgg16(pretrained=True)\n",
    "        for idx, layer in enumerate(vgg16_pretrained.features):\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                self.features[self.conv2deconv_indices[idx]].weight.data = layer.weight.data\n",
    "                #self.features[self.conv2deconv_indices[idx]].bias.data\\\n",
    "                # = layer.bias.data\n",
    "        \n",
    "    def forward(self, x, layer, activation_idx, pool_locs):\n",
    "        if layer in self.conv2deconv_indices:\n",
    "            start_idx = self.conv2deconv_indices[layer]\n",
    "        else:\n",
    "            raise ValueError('layer is not a conv feature map')\n",
    "\n",
    "        for idx in range(start_idx, len(self.features)):\n",
    "            if isinstance(self.features[idx], nn.MaxUnpool2d):\n",
    "                x = self.features[idx]\\\n",
    "                (x, pool_locs[self.unpool2pool_indices[idx]])\n",
    "            else:\n",
    "                x = self.features[idx](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92e2a771-4d82-4c82-97e8-1ada3d2c30bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision.transforms import transforms\n",
    "import numpy as np\n",
    "import cv2\n",
    "from functools import partial\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_images(img_path):\n",
    "    # imread from img_path\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "\n",
    "    # pytorch must normalize the pic by \n",
    "    # mean = [0.485, 0.456, 0.406]\n",
    "    # std = [0.229, 0.224, 0.225]\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    img = transform(img)\n",
    "    img.unsqueeze_(0)\n",
    "    #img_s = img.numpy()\n",
    "    #img_s = np.transpose(img_s, (1, 2, 0))\n",
    "    #cv2.imshow(\"test img\", img_s)\n",
    "    #cv2.waitKey()\n",
    "    return img\n",
    "\n",
    "def store(model):\n",
    "    \"\"\"\n",
    "    make hook for feature map\n",
    "    \"\"\"\n",
    "    def hook(module, input, output, key):\n",
    "        if isinstance(module, nn.MaxPool2d):\n",
    "           model.feature_maps[key] = output[0]\n",
    "           model.pool_locs[key] = output[1]\n",
    "        else:\n",
    "           model.feature_maps[key] = output\n",
    "    \n",
    "    for idx, layer in enumerate(model._modules.get('features')):    \n",
    "        # _modules returns an OrderedDict\n",
    "        layer.register_forward_hook(partial(hook, key=idx))\n",
    "\n",
    "def vis_layer(layer, vgg16_conv, vgg16_deconv, border_width=5):\n",
    "    \"\"\"\n",
    "    visualing the layer deconv result\n",
    "    \"\"\"\n",
    "    num_feat = vgg16_conv.feature_maps[layer].shape[1]\n",
    "    \n",
    "    # set other feature map activations to zero\n",
    "    new_feat_map = vgg16_conv.feature_maps[layer].clone()\n",
    "\n",
    "    # choose the max activations map\n",
    "    act_lst = []\n",
    "    for i in range(0, num_feat):\n",
    "        choose_map = new_feat_map[0, i, :, :]\n",
    "        activation = torch.max(choose_map)\n",
    "        act_lst.append(activation.item())\n",
    "\n",
    "    act_lst = np.array(act_lst)\n",
    "    mark = np.argmax(act_lst)\n",
    "\n",
    "    choose_map = new_feat_map[0, mark, :, :]\n",
    "    max_activation = torch.max(choose_map)\n",
    "    \n",
    "    # make zeros for other feature maps\n",
    "    if mark == 0:\n",
    "        new_feat_map[:, 1:, :, :] = 0\n",
    "    else:\n",
    "        new_feat_map[:, :mark, :, :] = 0\n",
    "        if mark != vgg16_conv.feature_maps[layer].shape[1] - 1:\n",
    "            new_feat_map[:, mark + 1:, :, :] = 0\n",
    "    \n",
    "    choose_map = torch.where(choose_map==max_activation,\n",
    "            choose_map,\n",
    "            torch.zeros(choose_map.shape)\n",
    "            )\n",
    "\n",
    "    # make zeros for ther activations\n",
    "    new_feat_map[0, mark, :, :] = choose_map\n",
    "    \n",
    "    # print(torch.max(new_feat_map[0, mark, :, :]))    \n",
    "    print(max_activation)\n",
    "    \n",
    "    deconv_output = vgg16_deconv(new_feat_map, layer, mark, vgg16_conv.pool_locs)\n",
    "\n",
    "    new_img = deconv_output.data.numpy()[0].transpose(1, 2, 0)  # (H, W, C)\n",
    "    # normalize\n",
    "    new_img = (new_img - new_img.min()) / (new_img.max() - new_img.min()) * 255\n",
    "    new_img = new_img.astype(np.uint8)\n",
    "  \n",
    "    # OpenCV处理\n",
    "    if new_img.shape[2] == 3:\n",
    "        gray_img = cv2.cvtColor(new_img, cv2.COLOR_RGB2GRAY)\n",
    "    else:\n",
    "        gray_img = new_img.squeeze()\n",
    "    \n",
    "    # 二值化\n",
    "    _, binary_img = cv2.threshold(gray_img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # 转换为三通道\n",
    "    binary_rgb = cv2.cvtColor(binary_img, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    # 添加黑色边框\n",
    "    h, w = binary_rgb.shape[:2]\n",
    "    border_color = [0, 0, 0]  # 纯黑\n",
    "    \n",
    "    # 方法1：使用copyMakeBorder（推荐）\n",
    "    bordered_img = cv2.copyMakeBorder(\n",
    "        binary_rgb,\n",
    "        top=border_width,\n",
    "        bottom=border_width,\n",
    "        left=border_width,\n",
    "        right=border_width,\n",
    "        borderType=cv2.BORDER_CONSTANT,\n",
    "        value=border_color\n",
    "    )\n",
    "  \n",
    "    return bordered_img, int(max_activation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcd2ea68-b74c-407a-80ba-c6d336bc324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def test(img_dir):\n",
    "    # 获取目录中的所有图片文件\n",
    "    img_files = [f for f in os.listdir(img_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))]\n",
    "    \n",
    "    # 随机选择9张图片\n",
    "    if len(img_files) < 9:\n",
    "        raise ValueError(f\"目录中图片数量不足9张，当前只有 {len(img_files)} 张图片\")\n",
    "    \n",
    "    selected_imgs = random.sample(img_files, 9)\n",
    "    \n",
    "    # 准备forward和backward处理的网络\n",
    "    vgg16_conv = Vgg16Conv()\n",
    "    vgg16_conv.eval()\n",
    "    store(vgg16_conv)\n",
    "    \n",
    "    vgg16_deconv = Vgg16Deconv()\n",
    "    vgg16_deconv.eval()\n",
    "    \n",
    "    # 创建原始图像九宫格\n",
    "    plt.figure(figsize=(15, 15), dpi=100)\n",
    "    plt.title('Original Images')\n",
    "    \n",
    "    original_images = []\n",
    "    deconv_images = []\n",
    "    \n",
    "    # 处理每张图片\n",
    "    for i, img_file in enumerate(selected_imgs):\n",
    "        # 完整的图片路径\n",
    "        img_path = os.path.join(img_dir, img_file)\n",
    "        \n",
    "        # 读取原始图像\n",
    "        original_img = Image.open(img_path)\n",
    "        original_img = original_img.resize((224, 224))\n",
    "        original_img_array = np.array(original_img)\n",
    "        \n",
    "        # 前向传播\n",
    "        img = load_images(img_path)\n",
    "        conv_output = vgg16_conv(img)\n",
    "        \n",
    "        # 反卷积处理\n",
    "        layer = 28\n",
    "        img_deconv, activation = vis_layer(layer, vgg16_conv, vgg16_deconv)\n",
    "        \n",
    "        # 存储图像\n",
    "        original_images.append(original_img_array)\n",
    "        deconv_images.append(img_deconv)\n",
    "    \n",
    "    # 显示原始图像九宫格\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            idx = i * 3 + j\n",
    "            plt.subplot(3, 3, idx + 1)\n",
    "            plt.imshow(original_images[idx])\n",
    "            plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('original_images.jpg')\n",
    "    plt.close()\n",
    "    \n",
    "    # 创建反卷积图像九宫格\n",
    "    plt.figure(figsize=(15, 15), dpi=100)\n",
    "    plt.title('Deconvolution Images ')\n",
    "    \n",
    "    # 显示反卷积图像九宫格\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            idx = i * 3 + j\n",
    "            plt.subplot(3, 3, idx + 1)\n",
    "            plt.imshow(deconv_images[idx])\n",
    "            plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('deconv_images.jpg')\n",
    "    plt.close()\n",
    "    \n",
    "    print('Images have been saved as original_images.jpg and deconv_images.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd27ac0b-dcb4-4746-982c-4a288f3641e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/environment/miniconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/environment/miniconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25.3790, grad_fn=<MaxBackward1>)\n",
      "tensor(19.1940, grad_fn=<MaxBackward1>)\n",
      "tensor(16.5878, grad_fn=<MaxBackward1>)\n",
      "tensor(21.8326, grad_fn=<MaxBackward1>)\n",
      "tensor(23.9670, grad_fn=<MaxBackward1>)\n",
      "tensor(27.2237, grad_fn=<MaxBackward1>)\n",
      "tensor(21.4932, grad_fn=<MaxBackward1>)\n",
      "tensor(14.7793, grad_fn=<MaxBackward1>)\n",
      "tensor(25.7451, grad_fn=<MaxBackward1>)\n",
      "Images have been saved as original_images.jpg and deconv_images.jpg\n"
     ]
    }
   ],
   "source": [
    "img_dir = 'dataset/n02105505'\n",
    "test(img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676da043-509d-4701-a85f-1051a3d55616",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
